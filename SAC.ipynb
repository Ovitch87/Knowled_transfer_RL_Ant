{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqrBrhgKxrHi"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4_BsJFDc82g6"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y libosmesa6-dev patchelf\n",
        "!pip install gymnasium[mujoco] imageio[ffmpeg] pyopengl glfw -qU\n",
        "!pip install wandb -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3MQycD08vjy"
      },
      "outputs": [],
      "source": [
        "!pip install stable-baselines3 -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GqgpJ5k8-gf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"MUJOCO_GL\"] = \"egl\"\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "\n",
        "import math\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import glob\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "from typing import Callable ###for evaluate\n",
        "from google.colab import files ###for downloading files\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import MultivariateNormal\n",
        "\n",
        "\n",
        "\n",
        "import stable_baselines3 as sb3\n",
        "\n",
        "if sb3.__version__ < \"2.0\":\n",
        "    raise ValueError(\n",
        "        \"\"\"Ongoing migration: run the following command to install the new dependencies:\n",
        "           poetry run pip install \"stable_baselines3==2.0.0a1\"\n",
        "        \"\"\"\n",
        "        )\n",
        "from stable_baselines3.common.buffers import ReplayBuffer\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kLBqw1O9AoE"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from torch.distributions.normal import Normal\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6T8Uyeiz9Aqy"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kr7iLQVN9AtZ"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Args:\n",
        "    exp_name: str = \"Climber_EXP_3\"\n",
        "    exp_group: str = \"Climber_EXP\"\n",
        "    \"\"\"the name of this experiment\"\"\"\n",
        "    seed: int = 3\n",
        "    \"\"\"seed of the experiment\"\"\"\n",
        "    torch_deterministic: bool = True\n",
        "    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n",
        "    cuda: bool = True\n",
        "    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n",
        "    track: bool = True\n",
        "    \"\"\"if toggled, this experiment will be tracked with Weights and Biases\"\"\"\n",
        "    wandb_project_name: str = \"FINAL_TL_EXP\"\n",
        "    \"\"\"the wandb's project name\"\"\"\n",
        "    wandb_entity: str = None\n",
        "    \"\"\"the entity (team) of wandb's project\"\"\"\n",
        "    capture_video: bool = False\n",
        "    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n",
        "\n",
        "    save_model: bool = True\n",
        "    load_model: bool = True\n",
        "\n",
        "    # env_id: str =  \"MountainCarContinuous-v0\"\n",
        "    # env_id: str =  \"Pendulum-v1\"\n",
        "    # env_id: str =  \"BipedalWalker-v3\"\n",
        "    # env_id: str =  \"LunarLanderContinuous-v2\"\n",
        "    # env_id: str =  \"InvertedPendulumSwingup-v4\"\n",
        "    # env_id: str =  \"InvertedDoublePendulumSwingup-v4\"\n",
        "    # env_id: str =  \"HalfCheetah-v5\"\n",
        "    # env_id: str =  \"Hopper-v5\"\n",
        "    # env_id: str =  \"Walker2d-v5\"\n",
        "\n",
        "    env_id: str =  \"Ant-v5\"\n",
        "    # env_id: str =  \"3LegAnt\"\n",
        "    # env_id: str =  \"5LegAnt\"\n",
        "    # env_id: str =  \"HopperAnt\"\n",
        "    # env_id: str =  \"LongShortAnt\"\n",
        "    # env_id: str =  \"ShortLongAnt\"\n",
        "\n",
        "    # env_id: str =  \"ClimberAnt\"\n",
        "    # env_id: str =  \"GoAroundAnt\"\n",
        "\n",
        "    # env_id: str =  \"Humanoid-v5\"\n",
        "    # env_id: str =  \"InvertedDoublePendulum-v5\"\n",
        "    # env_id: str =  \"InvertedPendulum-v5\"\n",
        "    # env_id: str =  \"Reacher-v5\"\n",
        "    # env_id: str =  \"Swimmer-v5\"\n",
        "    \"\"\"the environment id \"\"\"\n",
        "    total_timesteps: int = 500000\n",
        "    \"\"\"total timesteps of the experiments\"\"\"\n",
        "    num_envs: int = 5\n",
        "    \"\"\"the number of parallel game environments\"\"\"\n",
        "    buffer_size: int = int(1e6)\n",
        "    \"\"\"the replay memory buffer size\"\"\"\n",
        "    gamma: float = 0.99\n",
        "    \"\"\"the discount factor gamma\"\"\"\n",
        "    tau: float = 0.005\n",
        "    \"\"\"target smoothing coefficient (default: 0.005)\"\"\"\n",
        "    batch_size: int = 256\n",
        "    \"\"\"the batch size of sample from the reply memory\"\"\"\n",
        "    learning_starts: int = 5e3\n",
        "    \"\"\"timestep to start learning\"\"\"\n",
        "    policy_lr: float = 3e-4\n",
        "    \"\"\"the learning rate of the policy network optimizer\"\"\"\n",
        "    q_lr: float = 1e-3\n",
        "    \"\"\"the learning rate of the Q network network optimizer\"\"\"\n",
        "    policy_frequency: int = 2\n",
        "    \"\"\"the frequency of training policy (delayed)\"\"\"\n",
        "    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.\n",
        "    \"\"\"the frequency of updates for the target nerworks\"\"\"\n",
        "    alpha: float = 0.2\n",
        "    \"\"\"Entropy regularization coefficient.\"\"\"\n",
        "    autotune: bool = True\n",
        "    \"\"\"automatic tuning of the entropy coefficient\"\"\"\n",
        "\n",
        "\n",
        "def make_env(env_id, seed, idx, capture_video, run_name, xml=None):\n",
        "    def thunk():\n",
        "        if xml is not None:\n",
        "          if capture_video and idx == 0:\n",
        "              env = gym.make(env_id, xml_file=xml , render_mode=\"rgb_array\")\n",
        "          else:\n",
        "              env = gym.make(env_id, xml_file=xml)\n",
        "        else:\n",
        "          if capture_video and idx == 0:\n",
        "              env = gym.make(env_id, render_mode=\"rgb_array\")\n",
        "          else:\n",
        "              env = gym.make(env_id)\n",
        "\n",
        "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "        env.action_space.seed(seed)\n",
        "        return env\n",
        "\n",
        "    return thunk\n",
        "\n",
        "\n",
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    torch.nn.init.orthogonal_(layer.weight, std)\n",
        "    torch.nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer\n",
        "\n",
        "def xav_layer_init(layer, gain=1.0):\n",
        "    torch.nn.init.xavier_uniform_(layer.weight, gain=gain)\n",
        "    torch.nn.init.constant_(layer.bias, 0)\n",
        "    return layer\n",
        "\n",
        "def kaiming_layer_init(layer):\n",
        "    init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
        "    torch.nn.init.constant_(layer.bias, 0)\n",
        "    return layer\n",
        "\n",
        "# ALGO LOGIC: initialize agent here:\n",
        "class SoftQNetwork(nn.Module):\n",
        "    def __init__(self, env):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(\n",
        "            np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape),\n",
        "            256,\n",
        "        )\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc3 = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, x, a):\n",
        "        x = torch.cat([x, a], 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "LOG_STD_MAX = 2\n",
        "LOG_STD_MIN = -5\n",
        "\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, env):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))\n",
        "        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))\n",
        "\n",
        "        # action rescaling\n",
        "        self.register_buffer(\n",
        "            \"action_scale\",\n",
        "            torch.tensor(\n",
        "                (env.single_action_space.high - env.single_action_space.low) / 2.0,\n",
        "                dtype=torch.float32,\n",
        "            ),\n",
        "        )\n",
        "        self.register_buffer(\n",
        "            \"action_bias\",\n",
        "            torch.tensor(\n",
        "                (env.single_action_space.high + env.single_action_space.low) / 2.0,\n",
        "                dtype=torch.float32,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        mean = self.fc_mean(x)\n",
        "        log_std = self.fc_logstd(x)\n",
        "        log_std = torch.tanh(log_std)\n",
        "        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats\n",
        "\n",
        "        return mean, log_std\n",
        "\n",
        "    def get_action(self, x):\n",
        "        mean, log_std = self(x)\n",
        "        std = log_std.exp()\n",
        "        normal = torch.distributions.Normal(mean, std)\n",
        "        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))\n",
        "        y_t = torch.tanh(x_t)\n",
        "        action = y_t * self.action_scale + self.action_bias\n",
        "        log_prob = normal.log_prob(x_t)\n",
        "        # Enforcing Action Bound\n",
        "        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)\n",
        "        log_prob = log_prob.sum(1, keepdim=True)\n",
        "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
        "        return action, log_prob, mean\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "    model_path: str,\n",
        "    make_env: Callable,\n",
        "    env_id: str,\n",
        "    eval_episodes: int,\n",
        "    run_name: str,\n",
        "    Model: torch.nn.Module,\n",
        "    device: torch.device = torch.device(\"cpu\"),\n",
        "    capture_video: bool = True,\n",
        "    gamma: float = 0.99,\n",
        "    ):\n",
        "\n",
        "    envs = gym.vector.SyncVectorEnv([make_env(env_id, 0, capture_video, run_name, gamma)])\n",
        "    agent = Model(envs).to(device)\n",
        "    agent.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    agent.eval()\n",
        "\n",
        "    obs, _ = envs.reset()\n",
        "\n",
        "    episode_return = np.zeros(1, dtype=np.float32)\n",
        "    episodic_returns = []\n",
        "    while len(episodic_returns) < eval_episodes:\n",
        "        actions, _, _, _ = agent.get_action_and_value(torch.Tensor(obs).to(device))\n",
        "        obs, rewards, done, truncated, _ = envs.step(actions.detach().cpu().numpy())\n",
        "        dones = np.logical_or(done, truncated)\n",
        "        episode_return += rewards\n",
        "        if dones.any():\n",
        "          episodic_returns.append(episode_return)\n",
        "          episode_return = 0.0\n",
        "\n",
        "    return episodic_returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vs7C97_1r-ZI"
      },
      "outputs": [],
      "source": [
        "from gymnasium.envs.mujoco.ant_v5 import AntEnv\n",
        "from gymnasium.spaces import Box\n",
        "from gymnasium.envs.registration import register\n",
        "\n",
        "class ThreeLegAntEnv(AntEnv):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.action_space = Box(low=-1.0, high=1.0, shape=(6,), dtype=np.float32)\n",
        "\n",
        "register(\n",
        "    id=\"3LegAnt\",\n",
        "    entry_point=ThreeLegAntEnv,\n",
        "    max_episode_steps=1000,\n",
        "    reward_threshold=6000.0,\n",
        ")\n",
        "\n",
        "class FiveLegAntEnv(AntEnv):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.action_space = Box(low=-1.0, high=1.0, shape=(10,), dtype=np.float32)\n",
        "\n",
        "register(\n",
        "    id=\"5LegAnt\",\n",
        "    entry_point=FiveLegAntEnv,\n",
        "    max_episode_steps=1000,\n",
        "    reward_threshold=6000.0,\n",
        ")\n",
        "\n",
        "class HopperAntEnv(AntEnv):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "register(\n",
        "    id=\"HopperAnt\",\n",
        "    entry_point=HopperAntEnv,\n",
        "    max_episode_steps=1000,\n",
        "    reward_threshold=6000.0,\n",
        ")\n",
        "\n",
        "class LongShortAntEnv(AntEnv):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "register(\n",
        "    id=\"LongShortAnt\",\n",
        "    entry_point=LongShortAntEnv,\n",
        "    max_episode_steps=1000,\n",
        "    reward_threshold=6000.0,\n",
        ")\n",
        "\n",
        "class ShortLongAntEnv(AntEnv):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "register(\n",
        "    id=\"ShortLongAnt\",\n",
        "    entry_point=ShortLongAntEnv,\n",
        "    max_episode_steps=1000,\n",
        "    reward_threshold=6000.0,\n",
        ")\n",
        "\n",
        "class ClimberAntEnv(AntEnv):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self._forward_reward_weight = 0.3\n",
        "    self._healthy_z_range = (0.2, 1.6)\n",
        "  def _get_rew(self, x_velocity: float, action):\n",
        "      to_x_reward = 0\n",
        "      if x_velocity > 0.3:\n",
        "        to_x_reward = 2\n",
        "      forward_reward = (x_velocity * self._forward_reward_weight) + to_x_reward\n",
        "      healthy_reward = self.healthy_reward\n",
        "      rewards = forward_reward + healthy_reward\n",
        "\n",
        "      ctrl_cost = self.control_cost(action)\n",
        "      contact_cost = self.contact_cost\n",
        "      costs = ctrl_cost + contact_cost\n",
        "\n",
        "      reward = rewards - costs\n",
        "\n",
        "      reward_info = {\n",
        "          \"reward_forward\": forward_reward,\n",
        "          \"reward_ctrl\": -ctrl_cost,\n",
        "          \"reward_contact\": -contact_cost,\n",
        "          \"reward_survive\": healthy_reward,\n",
        "      }\n",
        "\n",
        "      return reward, reward_info\n",
        "\n",
        "register(\n",
        "    id=\"ClimberAnt\",\n",
        "    entry_point=ClimberAntEnv,\n",
        "    max_episode_steps=1200,\n",
        "    reward_threshold=5000.0,\n",
        ")\n",
        "\n",
        "class GoAroundAntEnv(AntEnv):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self._forward_reward_weight = 0.3\n",
        "  def _get_rew(self, x_velocity: float, action):\n",
        "      to_x_reward = 0\n",
        "      if x_velocity > 0.3:\n",
        "        to_x_reward = 2\n",
        "      forward_reward = (x_velocity * self._forward_reward_weight) + to_x_reward\n",
        "      healthy_reward = self.healthy_reward\n",
        "      rewards = forward_reward + healthy_reward\n",
        "\n",
        "      ctrl_cost = self.control_cost(action)\n",
        "      contact_cost = self.contact_cost\n",
        "      costs = ctrl_cost + contact_cost\n",
        "\n",
        "      reward = rewards - costs\n",
        "\n",
        "      reward_info = {\n",
        "          \"reward_forward\": forward_reward,\n",
        "          \"reward_ctrl\": -ctrl_cost,\n",
        "          \"reward_contact\": -contact_cost,\n",
        "          \"reward_survive\": healthy_reward,\n",
        "      }\n",
        "\n",
        "      return reward, reward_info\n",
        "\n",
        "\n",
        "register(\n",
        "    id=\"GoAroundAnt\",\n",
        "    entry_point=GoAroundAntEnv,\n",
        "    max_episode_steps=1200,\n",
        "    reward_threshold=5000.0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDHGAAPkFlli"
      },
      "outputs": [],
      "source": [
        "from gymnasium.vector.vector_env import AutoresetMode\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrvjsDGivBwv"
      },
      "outputs": [],
      "source": [
        "def train(seed=None):\n",
        "    args = Args()\n",
        "    if seed is not None:\n",
        "      args.seed = seed\n",
        "    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
        "    if args.track:\n",
        "        import wandb\n",
        "\n",
        "        wandb.init(\n",
        "            project=args.wandb_project_name,\n",
        "            entity=args.wandb_entity,\n",
        "            sync_tensorboard=True,\n",
        "            config=vars(args),\n",
        "            name=run_name,\n",
        "            group=args.exp_group, ###\n",
        "            monitor_gym=True,\n",
        "            save_code=True,\n",
        "            reinit=\"return_previous\" ###\n",
        "        )\n",
        "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
        "    writer.add_text(\n",
        "        \"hyperparameters\",\n",
        "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
        "    )\n",
        "\n",
        "    # TRY NOT TO MODIFY: seeding\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
        "\n",
        "    ################## get xml path ##################\n",
        "    xml_path = None\n",
        "    if args.env_id != \"Ant-v5\":\n",
        "      xml_path = f'/content/{args.env_id}.xml'\n",
        "    ##################################################\n",
        "    # env setup\n",
        "    envs = gym.vector.AsyncVectorEnv(\n",
        "        [make_env(args.env_id, args.seed + i, i, args.capture_video, run_name, xml=xml_path) for i in range(args.num_envs)],\n",
        "        autoreset_mode=AutoresetMode.SAME_STEP\n",
        "    )\n",
        "    assert isinstance(envs.single_action_space, gym.spaces.Box), \"only continuous action space is supported\"\n",
        "\n",
        "    max_action = float(envs.single_action_space.high[0])\n",
        "\n",
        "    actor = Actor(envs).to(device)\n",
        "    qf1 = SoftQNetwork(envs).to(device)\n",
        "    qf2 = SoftQNetwork(envs).to(device)\n",
        "    qf1_target = SoftQNetwork(envs).to(device)\n",
        "    qf2_target = SoftQNetwork(envs).to(device)\n",
        "    qf1_target.load_state_dict(qf1.state_dict())\n",
        "    qf2_target.load_state_dict(qf2.state_dict())\n",
        "    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)\n",
        "    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)\n",
        "\n",
        "    ######## load pretrained model #########\n",
        "    if args.load_model:\n",
        "      for file in os.listdir('/content'):\n",
        "        if file.endswith(\"_actor.pth\"):\n",
        "          filepath = os.path.join('/content', file)\n",
        "          actor.load_state_dict(torch.load(filepath, map_location=device))\n",
        "        elif file.endswith(\"_qf1.pth\"):\n",
        "          filepath = os.path.join('/content', file)\n",
        "          qf1.load_state_dict(torch.load(filepath, map_location=device))\n",
        "        elif file.endswith(\"_qf2.pth\"):\n",
        "          filepath = os.path.join('/content', file)\n",
        "          qf2.load_state_dict(torch.load(filepath, map_location=device))\n",
        "        elif file.endswith(\"_qf1_target.pth\"):\n",
        "          filepath = os.path.join('/content', file)\n",
        "          qf1_target.load_state_dict(torch.load(filepath, map_location=device))\n",
        "        elif file.endswith(\"_qf2_target.pth\"):\n",
        "          filepath = os.path.join('/content', file)\n",
        "          qf2_target.load_state_dict(torch.load(filepath, map_location=device))\n",
        "    ########################################\n",
        "\n",
        "    ########################## log model ######################\n",
        "    if args.track:\n",
        "        wandb.watch(actor, log=\"all\", log_freq=1024)\n",
        "        model_architecture = str(actor)\n",
        "        wandb.log({f\"model/architecture-exp.:{args.exp_name}-actor\": wandb.Html(f\"<pre>{model_architecture}</pre>\")})\n",
        "        wandb.watch(qf1, log=\"all\", log_freq=1024)\n",
        "        model_architecture = str(qf1)\n",
        "        wandb.log({f\"model/architecture-exp.:{args.exp_name}-SoftQnet\": wandb.Html(f\"<pre>{model_architecture}</pre>\")})\n",
        "        wandb.watch(qf2, log=\"all\", log_freq=1024)\n",
        "        wandb.watch(qf1_target, log=\"all\", log_freq=1024)\n",
        "        wandb.watch(qf2_target, log=\"all\", log_freq=1024)\n",
        "    ############################################################\n",
        "\n",
        "    # Automatic entropy tuning\n",
        "    if args.autotune:\n",
        "        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()\n",
        "        log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
        "        alpha = log_alpha.exp().item()\n",
        "        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)\n",
        "    else:\n",
        "        alpha = args.alpha\n",
        "\n",
        "    envs.single_observation_space.dtype = np.float32\n",
        "    rb = ReplayBuffer(\n",
        "        args.buffer_size,\n",
        "        envs.single_observation_space,\n",
        "        envs.single_action_space,\n",
        "        device,\n",
        "        n_envs=args.num_envs,\n",
        "        handle_timeout_termination=False,\n",
        "    )\n",
        "    start_time = time.time()\n",
        "\n",
        "    ######## for logging reward components #############\n",
        "    reward_components = [\"reward_forward\", \"reward_ctrl\", \"reward_contact\", \"reward_survive\"]\n",
        "    reward_sums = {comp: np.zeros(args.num_envs, dtype=np.float32) for comp in reward_components}\n",
        "    smoothed_returns = deque(maxlen=50)\n",
        "    ####################################################\n",
        "\n",
        "    # TRY NOT TO MODIFY: start the game\n",
        "    obs, _ = envs.reset(seed=args.seed)\n",
        "    for global_step in range(args.total_timesteps):\n",
        "        # ALGO LOGIC: put action logic here\n",
        "        if global_step < args.learning_starts:\n",
        "            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])\n",
        "        else:\n",
        "            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))\n",
        "            actions = actions.detach().cpu().numpy()\n",
        "\n",
        "        # TRY NOT TO MODIFY: execute the game and log data.\n",
        "        next_obs, rewards, terminations, truncations, infos = envs.step(actions)\n",
        "        # print(infos)\n",
        "\n",
        "        ###### calc reward comp ####\n",
        "        for comp in reward_components:\n",
        "          if comp in infos:\n",
        "            reward_sums[comp] += infos[comp]\n",
        "        ############################\n",
        "\n",
        "        # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
        "        if \"final_info\" in infos:\n",
        "            # print(\"infos['final_info']:\", infos[\"final_info\"])\n",
        "            episode_infos = infos[\"final_info\"][\"episode\"]\n",
        "            episode_mask = infos[\"final_info\"][\"_episode\"]\n",
        "            episode_returns = episode_infos[\"r\"][episode_mask]\n",
        "            episode_lengths = episode_infos[\"l\"][episode_mask]\n",
        "            avg_epi_return = np.mean(episode_returns)\n",
        "            avg_epi_length = np.mean(episode_lengths)\n",
        "            # Append to smoothing buffer\n",
        "            smoothed_returns.extend(episode_returns)\n",
        "            smoothed_avg_return = np.mean(smoothed_returns)\n",
        "            ####\n",
        "            print(f\"global_step: {global_step*args.num_envs}, avg_episodic_return: {avg_epi_return} avg_episodic_length: {avg_epi_length}\")\n",
        "            writer.add_scalar(\"charts/envs_finished\", len(episode_returns), (global_step*args.num_envs))\n",
        "            writer.add_scalar(\"charts/avg_episodic_return\", avg_epi_return,(global_step*args.num_envs))\n",
        "            writer.add_scalar(\"charts/smoothed_avg_episodic_return\", smoothed_avg_return, global_step * args.num_envs)\n",
        "            writer.add_scalar(\"charts/avg_episodic_length\", avg_epi_length, (global_step*args.num_envs))\n",
        "            for comp in reward_components:\n",
        "              avg_reward_comp = np.mean(reward_sums[comp][episode_mask])\n",
        "              writer.add_scalar(f\"costs_reward/{comp}\", avg_reward_comp, (global_step*args.num_envs))\n",
        "              reward_sums[comp][episode_mask] = 0.0\n",
        "\n",
        "        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`\n",
        "        real_next_obs = next_obs.copy()\n",
        "        for idx, trunc in enumerate(truncations):\n",
        "            if trunc:\n",
        "                real_next_obs[idx] = infos[\"final_obs\"][idx]\n",
        "        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)\n",
        "\n",
        "        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook\n",
        "        obs = next_obs\n",
        "\n",
        "        # ALGO LOGIC: training.\n",
        "        if global_step > args.learning_starts:\n",
        "            data = rb.sample(args.batch_size)\n",
        "            with torch.no_grad():\n",
        "                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)\n",
        "                qf1_next_target = qf1_target(data.next_observations, next_state_actions)\n",
        "                qf2_next_target = qf2_target(data.next_observations, next_state_actions)\n",
        "                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi\n",
        "                next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)\n",
        "\n",
        "            qf1_a_values = qf1(data.observations, data.actions).view(-1)\n",
        "            qf2_a_values = qf2(data.observations, data.actions).view(-1)\n",
        "            qf1_loss = F.mse_loss(qf1_a_values, next_q_value)\n",
        "            qf2_loss = F.mse_loss(qf2_a_values, next_q_value)\n",
        "            qf_loss = qf1_loss + qf2_loss\n",
        "\n",
        "            # optimize the model\n",
        "            q_optimizer.zero_grad()\n",
        "            qf_loss.backward()\n",
        "            q_optimizer.step()\n",
        "\n",
        "            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support\n",
        "                for _ in range(\n",
        "                    args.policy_frequency\n",
        "                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1\n",
        "                    pi, log_pi, _ = actor.get_action(data.observations)\n",
        "                    if args.track:\n",
        "                      entropy = -log_pi.mean().item()\n",
        "                      writer.add_scalar(\"policy/entropy\", entropy, global_step * args.num_envs)\n",
        "                    qf1_pi = qf1(data.observations, pi)\n",
        "                    qf2_pi = qf2(data.observations, pi)\n",
        "                    min_qf_pi = torch.min(qf1_pi, qf2_pi)\n",
        "                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()\n",
        "\n",
        "                    actor_optimizer.zero_grad()\n",
        "                    actor_loss.backward()\n",
        "                    actor_optimizer.step()\n",
        "\n",
        "                    if args.autotune:\n",
        "                        with torch.no_grad():\n",
        "                            _, log_pi, _ = actor.get_action(data.observations)\n",
        "                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()\n",
        "\n",
        "                        a_optimizer.zero_grad()\n",
        "                        alpha_loss.backward()\n",
        "                        a_optimizer.step()\n",
        "                        alpha = log_alpha.exp().item()\n",
        "\n",
        "            # update the target networks\n",
        "            if global_step % args.target_network_frequency == 0:\n",
        "                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):\n",
        "                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
        "                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):\n",
        "                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
        "\n",
        "            if global_step % 100 == 0:\n",
        "                writer.add_scalar(\"charts/train_minutes\", ((time.time() - start_time)//60), (global_step*args.num_envs)) ########\n",
        "                writer.add_scalar(\"losses/qf1_values\", qf1_a_values.mean().item(), (global_step*args.num_envs))\n",
        "                writer.add_scalar(\"losses/qf2_values\", qf2_a_values.mean().item(), (global_step*args.num_envs))\n",
        "                writer.add_scalar(\"losses/qf1_loss\", qf1_loss.item(), (global_step*args.num_envs))\n",
        "                writer.add_scalar(\"losses/qf2_loss\", qf2_loss.item(), (global_step*args.num_envs))\n",
        "                writer.add_scalar(\"losses/qf_loss\", qf_loss.item() / 2.0, (global_step*args.num_envs))\n",
        "                writer.add_scalar(\"losses/actor_loss\", actor_loss.item(), (global_step*args.num_envs))\n",
        "                writer.add_scalar(\"losses/alpha\", alpha, (global_step*args.num_envs))\n",
        "                print(\"SPS:\", int((global_step*args.num_envs) / (time.time() - start_time)), \" total_run_time:\", ((time.time()-start_time)//60),\" minutes\")\n",
        "                writer.add_scalar(\n",
        "                    \"charts/SPS\",\n",
        "                    int((global_step*args.num_envs) / (time.time() - start_time)),\n",
        "                    (global_step*args.num_envs),\n",
        "                )\n",
        "                if args.autotune:\n",
        "                    writer.add_scalar(\"losses/alpha_loss\", alpha_loss.item(), (global_step*args.num_envs))\n",
        "    if args.save_model:\n",
        "        model_path = f\"runs/{run_name}/{args.exp_name}-seed-{args.seed}\"\n",
        "        os.makedirs(model_path, exist_ok=True)\n",
        "        torch.save(actor.state_dict(), f\"{model_path}/{seed}_{args.exp_name}_actor.pth\")\n",
        "        torch.save(qf1.state_dict(), f\"{model_path}/{seed}_{args.exp_name}_qf1.pth\")\n",
        "        torch.save(qf2.state_dict(), f\"{model_path}/{seed}_{args.exp_name}_qf2.pth\")\n",
        "        torch.save(qf1_target.state_dict(), f\"{model_path}/{seed}_{args.exp_name}_qf1_target.pth\")\n",
        "        torch.save(qf2_target.state_dict(), f\"{model_path}/{seed}_{args.exp_name}_qf2_target.pth\")\n",
        "        if args.autotune:\n",
        "            torch.save(log_alpha, f\"{model_path}/{seed}_{args.exp_name}_log_alpha.pth\")\n",
        "        print(f\"model saved to {model_path}\")\n",
        "\n",
        "    envs.close()\n",
        "    writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnvKF0xVP3Ve"
      },
      "outputs": [],
      "source": [
        "# for seed in [1,2,3,4,5]:\n",
        "#   train(seed)\n",
        "\n",
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihb705_a8J4p"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZydFY3hmFShX"
      },
      "outputs": [],
      "source": [
        "#download models\n",
        "for folder in os.listdir(f\"runs\"):\n",
        "  if os.path.isdir(os.path.join(f\"runs\", folder)):\n",
        "    for subfolder in os.listdir(os.path.join(f\"runs\", folder)):\n",
        "      if os.path.isdir(os.path.join(f\"runs\", folder, subfolder)):\n",
        "        for file in os.listdir(os.path.join(f\"runs\", folder, subfolder)):\n",
        "          for data in [\"actor\", \"qf1\", \"qf2\", \"qf1_target\", \"qf2_target\", \"log_alpha\"]:\n",
        "            if file.endswith(f\"{data}.pth\"):\n",
        "              filepath = os.path.join(f\"runs\", folder, subfolder, file)\n",
        "              files.download(filepath)\n",
        "              time.sleep(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFLQPLMx9Ayp"
      },
      "outputs": [],
      "source": [
        "#get animation\n",
        "plt.rcParams['animation.embed_limit'] = 1500 * 1024 * 1024\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "pretrained = False\n",
        "\n",
        "filepath = None\n",
        "if pretrained:\n",
        "  for file in os.listdir(f\"runs\"):\n",
        "    if file.endswith(\"actor.pth\"):\n",
        "      filepath = os.path.join(f\"runs\", file)\n",
        "else:\n",
        "  for folder in os.listdir(f\"runs\"):\n",
        "    if os.path.isdir(os.path.join(f\"runs\", folder)):\n",
        "      for subfolder in os.listdir(os.path.join(f\"runs\", folder)):\n",
        "        if os.path.isdir(os.path.join(f\"runs\", folder, subfolder)):\n",
        "          for file in os.listdir(os.path.join(f\"runs\", folder, subfolder)):\n",
        "            if file.endswith(\"actor.pth\"):\n",
        "              filepath = os.path.join(f\"runs\", folder, subfolder, file)\n",
        "\n",
        "args = Args()\n",
        "\n",
        "xml_path = None\n",
        "if args.env_id != \"Ant-v5\":\n",
        "    xml_path = f'/content/{args.env_id}.xml'\n",
        "\n",
        "envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, True, None, xml=xml_path)])\n",
        "actor = Actor(envs).to(device)\n",
        "actor.load_state_dict(torch.load(filepath, map_location=device))\n",
        "actor.eval()\n",
        "\n",
        "obs, _ = envs.reset(seed=args.seed)\n",
        "obs = torch.Tensor(obs).to(device)\n",
        "frames = []\n",
        "terminations = False\n",
        "truncations = False\n",
        "for _ in range(800):\n",
        "    action, _, _ = actor.get_action(torch.Tensor(obs).to(device))\n",
        "    action = action.detach().cpu().numpy()\n",
        "    obs, rewards, terminations, truncations, infos = envs.step(action)\n",
        "    obs = torch.Tensor(obs).to(device)\n",
        "    frames.append(envs.render()[0])\n",
        "    if terminations[0] or truncations[0]:\n",
        "      display(infos)\n",
        "      break\n",
        "\n",
        "envs.close()\n",
        "\n",
        "\n",
        "def display_video(frames, fps=24):\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.axis(\"off\")\n",
        "    img = ax.imshow(frames[0])\n",
        "\n",
        "    def update(frame):\n",
        "        img.set_array(frame)\n",
        "        return [img]\n",
        "\n",
        "    ani = FuncAnimation(fig, update, frames=frames, blit=True, interval=1000 // fps)\n",
        "    ani_file = f\"runs/{args.env_id}.mp4\"\n",
        "    ani.save(ani_file, writer='ffmpeg', fps=24)\n",
        "    files.download(ani_file)\n",
        "    plt.close(fig)\n",
        "    # return HTML(ani.to_jshtml())\n",
        "\n",
        "display_video(frames)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}